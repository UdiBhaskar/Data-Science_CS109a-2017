{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 109A/STAT 121A/AC 209A/CSCI E-109A \n",
    "\n",
    "# Lab 3: Linear Regression\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2017**<br>\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "\n",
    "<ol>\n",
    "<li> Linear regression with a toy  </li>\n",
    "- matrices and math <br>\n",
    "- building a model from scratch<br>\n",
    "- building a model with statsmodel and sklearn\n",
    "<li> Simple linear regression with automobile data </li>\n",
    "<li> Multiple linear regression with automobile data </li>\n",
    "<li> Interpreting results</li>\n",
    "</ol>\n",
    "\n",
    "*This lab maps on to lectures 3, 4, 5 and homework 2.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear regression with a toy \n",
    "We first examine a toy problem, focusing our efforts on fitting a linear model to a small dataset with three observations.  Each observation consists of one predictor $x_i$ and one response $y_i$ for $i = 1, 2, 3$,\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}.\n",
    "\\end{equation*}\n",
    "\n",
    "To be very concrete, let's set the values of the predictors and responses.\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(1, 2), (2, 2), (3, 4)\\}\n",
    "\\end{equation*}\n",
    "\n",
    "There is no line of the form $\\beta_0 + \\beta_1 x = y$ that passes through all three observations, since the data is not collinear.  Thus our aim is to find the line that best fits these observations in the *least-squares sense*, as discussed in lecture.\n",
    "\n",
    "\n",
    "### Matrices and math [10 minutes]\n",
    "\n",
    "Suspending reality, suppose there is a line $\\beta_0 + \\beta_1 x = y$ that passes through all three observations.  Then we'd solve\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta_0 + \\beta_1 &=& 2 \\nonumber \\\\\n",
    "\\beta_0 + 2 \\beta_1 &=& 2 \\nonumber \\\\\n",
    "\\beta_0 + 3 \\beta_1 &=& 4, \\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "for  $\\beta_0$ and  $\\beta_1$, the intercept and slope of the desired line.  Let's write these equations in matrix form.  The left hand sides of the above equations can be written as\n",
    "\n",
    "<img src=\"images/LHS.pdf\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "while the right hand side is simply the vector\n",
    "\n",
    "\\begin{equation*}Y = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "4 \n",
    "\\end{bmatrix}. \\end{equation*}\n",
    "\n",
    "Thus we have the matrix equation $X \\beta = Y$ where\n",
    "\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "1 & 2\\\\\n",
    "1 & 3\n",
    "\\end{bmatrix}, \\quad\n",
    "\\beta = \\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \n",
    "\\end{pmatrix}, \\quad \\mathrm{and} \n",
    "\\quad Y = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "4 \n",
    "\\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "To find the best possible solution to this linear system that has no solution, we need to solve the *normal equations*, or\n",
    "\n",
    "\\begin{equation}\n",
    "X^T X \\beta = X^T Y.\n",
    "\\end{equation}\n",
    "\n",
    "If $X^T X$ is invertible then the solution is\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = (X^T X)^{-1} X^T Y.\n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** What if the toy problem included a second predictor variable?  How would $X, \\beta$, and $Y$ change, if at all?  Would anything else change?  Create a new markdown cell below and explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "if we include it in the $X$ matrix as\n",
    "\n",
    "\\begin{equation}\n",
    "X = \\begin{bmatrix}\n",
    "1 & 1 & v_1\\\\\n",
    "1 & 2 & v_2\\\\\n",
    "1 & 3 & v_3\n",
    "\\end{bmatrix},\n",
    "\\end{equation}\n",
    "then the corresponding $\\beta$ vector is\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = \\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Thus the linear system in matrix form is still $X \\beta = Y$, \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & v_1\\\\\n",
    "1 & 2 & v_2\\\\\n",
    "1 & 3 & v_3\n",
    "\\end{bmatrix} \\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{pmatrix} = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "4 \n",
    "\\end{bmatrix}, \n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model from scratch [15 minutes]\n",
    "\n",
    "We now solve the normal equations to find the best fit solution to our toy problem.   Note that we have constructed our toy problem so that $X^T X$ is invertible.  Let's import the needed modules.  Note that we've imported statsmodels and sklearn in this below, which we'll use to build regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippets of code below solves the equations using the observed predictors and responses, which we'll call the training data set.  Let's walk through the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3L,)\n"
     ]
    }
   ],
   "source": [
    "#observed predictors\n",
    "x_train = np.array([1, 2, 3])\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3L, 1L)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(len(x_train),1)\n",
    "#check dimensions \n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3L, 1L)\n"
     ]
    }
   ],
   "source": [
    "#observed responses\n",
    "y_train = np.array([2, 2, 4])\n",
    "y_train = y_train.reshape(len(y_train),1)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 1.,  1.],\n",
      "       [ 1.,  2.],\n",
      "       [ 1.,  3.]]), (3L, 2L))\n"
     ]
    }
   ],
   "source": [
    "#build matrix X by concatenating predictors and a column of ones\n",
    "n = x_train.shape[0]\n",
    "ones_col = np.ones((n, 1))\n",
    "X = np.concatenate((ones_col, x_train), axis=1)\n",
    "#check X and dimensions\n",
    "print(X, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#matrix X^T X\n",
    "LHS = np.dot(np.transpose(X), X)\n",
    "\n",
    "#matrix X^T Y\n",
    "RHS = np.dot(np.transpose(X), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#solution beta to normal equations, since LHS is invertible by toy construction\n",
    "betas = np.dot(np.linalg.inv(LHS), RHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.66666667]), array([ 1.]))\n"
     ]
    }
   ],
   "source": [
    "#intercept beta0\n",
    "beta0 = betas[0]\n",
    "\n",
    "#slope beta1\n",
    "beta1 = betas[1]\n",
    "\n",
    "print(beta0, beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Turn the code from the above cells into a function, called `simple_linear_regression_fit`, that inputs the training data and returns `beta0` and `beta1`.\n",
    "\n",
    "> To do this, copy and paste the code from the above cells below and adjust the code as needed, so that the training data becomes the input and the betas become the output.\n",
    "\n",
    "> Check your function by calling it with the training data from above and printing out the beta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "def simple_linear_regression_fit(x_train, y_train):\n",
    "    x_train = x_train.reshape(len(x_train),1)\n",
    "    y_train = y_train.reshape(len(y_train),1)\n",
    "    n = x_train.shape[0]\n",
    "    ones_col = np.ones((n, 1))\n",
    "    X = np.concatenate((ones_col, x_train), axis=1)\n",
    "    LHS = np.dot(np.transpose(X), X)\n",
    "    RHS = np.dot(np.transpose(X), y_train)\n",
    "    betas = np.dot(np.linalg.inv(LHS), RHS)\n",
    "    return betas\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.66666667])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas1 = simple_linear_regression_fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Plot the training data.  Do the values of `beta0` and `beta1` seem reasonable?\n",
    "\n",
    ">Now write a lambda function `f` for the best fit line with `beta0` and `beta1`, and plot the best fit line together with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xc3aae80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEPCAYAAABY9lNGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1V4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc54ce80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code here\n",
    "beta0 = betas1[0]\n",
    "beta1 = betas1[1]\n",
    "f = lambda x : beta0 + beta1*x\n",
    "xfit = np.arange(0,4,.01)#.reshape(len(np.arange(0,4,.01)),1)\n",
    "yfit = f(xfit)\n",
    "#note that you don't have to reshape xfit as above and the plotting will still work!\n",
    "\n",
    "plt.plot(x_train, y_train, 'ko', xfit, yfit)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model with statsmodel and sklearn [10 minutes]\n",
    "\n",
    "Now that we can concretely fit the training data from scratch, let's learn two Python packages to do it all for us: [statsmodels](http://www.statsmodels.org/stable/regression.html) and [scikit-learn (sklearn)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).  Our goal  is to show how to implement simple linear regression with these packages.  For an important sanity check, we compare the $\\beta$ values from statsmodel and sklearn to the $\\beta$ values that we found from above from scratch.\n",
    "\n",
    "For the purposes of this lab, statsmodels and sklearn do the same thing.  More generally though, statsmodels tends to be easier for inference, whereas sklearn has machine-learning algorithms and is better for prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for statsmodels.  Statsmodels does not by default include the column of ones in the $X$ matrix, so we include it with `sm.add_constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 1.  2.]\n",
      " [ 1.  3.]]\n",
      "(beta0, beta1) = (0.666667, 1.000000)\n"
     ]
    }
   ],
   "source": [
    "#create the X matrix by appending a column of ones to x_train\n",
    "X = sm.add_constant(x_train)\n",
    "#this is the same matrix as in our scratch problem!\n",
    "print(X)\n",
    "#build the OLS model (ordinary least squares) from the training data\n",
    "toyregr_sm = sm.OLS(y_train, X)\n",
    "#save regression info (parameters, etc) in results_sm\n",
    "results_sm = toyregr_sm.fit()\n",
    "#pull the beta parameters out from results_sm\n",
    "beta0_sm = results_sm.params[0]\n",
    "beta1_sm = results_sm.params[1]\n",
    "\n",
    "print(\"(beta0, beta1) = (%f, %f)\" %(beta0_sm, beta1_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the beta parameters, `results_sm` contains a ton of other potentially useful information.  Type `results_sm` and hit tab to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(beta0, beta1) = (0.666667, 1.000000)\n"
     ]
    }
   ],
   "source": [
    "#build the least squares model\n",
    "toyregr_skl = linear_model.LinearRegression()\n",
    "#save regression info (parameters, etc) in results_skl\n",
    "results_skl = toyregr_skl.fit(x_train,y_train)\n",
    "#pull the beta parameters out from results_skl\n",
    "beta0_skl = toyregr_skl.intercept_\n",
    "beta1_skl = toyregr_skl.coef_[0]\n",
    "\n",
    "print(\"(beta0, beta1) = (%f, %f)\" %(beta0_skl, beta1_skl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should feel pretty good about ourselves now, and we're ready to move on to a real problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Simple linear regression with automobile data [30 minutes]\n",
    "We will now use sklearn to to predict automobile milesage per gallon (mpg) and evaluate these predictions. We first load the data and split them into a training set and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
       "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load mtcars\n",
    "dfcars=pd.read_csv(\"data/mtcars.csv\")\n",
    "dfcars=dfcars.rename(columns={\"Unnamed: 0\":\"name\"})\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into training set and testing set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#set random_state to get the same split every time\n",
    "traindf, testdf = train_test_split(dfcars, test_size=0.3, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 12), (22, 12), (10, 12))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing set is ~30% of the total data; training set is ~70%\n",
    "dfcars.shape, traindf.shape, testdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose the variables that we think will be good predictors for the dependent variable `mpg`.â€Š\n",
    "\n",
    ">**EXERCISE:**  Pick one variable to use as a predictor for simple linear regression.  Create a markdown cell below and discuss your reasons.  You may want to justify this with some visualizations.  Is there a second variable you'd like to use as well, say for multiple linear regression with two predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "hp and wt intuitively seem like reasonable predictors. For example, heavier cars are likely less fuel efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** With either sklearn or statsmodels, fit the training data using simple linear regression.  Use the model to make mpg predictions on testing set.  \n",
    "\n",
    "> Plot the data and the prediction.  \n",
    "\n",
    ">Print out the mean squared error for the training set and the testing set and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here\n",
    "#define  predictor and response for training set\n",
    "y_train = traindf.mpg\n",
    "X_train = traindf[['wt']]\n",
    "\n",
    "# define predictor and response for testing set\n",
    "y_test = testdf.mpg\n",
    "X_test = testdf[['wt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-5.2525741039089882, 37.777858413135093)\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "# create linear regression object with sklearn\n",
    "ling_test = linear_model.LinearRegression().fit(X_train,y_train)\n",
    "\n",
    "#your code here\n",
    "# train the model and make predictions\n",
    "pred = ling_test.predict(X_test)\n",
    "\n",
    "#your code here\n",
    "#print out coefficients\n",
    "print(ling_test.coef_[0], ling_test.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xcf075f8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd535e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test, color=\"black\")\n",
    "plt.plot(X_test, pred, color=\"blue\")\n",
    "plt.xlabel('wt')\n",
    "plt.ylabel('mpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training MSE is 7.308504, the testing MSE is 13.767534\n",
      "7.30850411454\n",
      "13.7675338276\n"
     ]
    }
   ],
   "source": [
    "train_MSE2= np.mean((y_train - ling_test.predict(X_train))**2)\n",
    "test_MSE2= np.mean((y_test - ling_test.predict(X_test))**2)\n",
    "print(\"The training MSE is %2f, the testing MSE is %2f\" %(train_MSE2, test_MSE2))\n",
    "\n",
    "#sklearn.metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_train, ling_test.predict(X_train)))\n",
    "print(mean_squared_error(y_test, ling_test.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multiple linear regression with automobile data [15 minutes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** With either sklearn or statsmodels, fit the training data using multiple linear regression with two predictors.  Use the model to make mpg predictions on testing set.  Print out the mean squared error for the training set and the testing set and compare.  \n",
    "\n",
    ">How do these training and testing mean squared errors compare to those from the simple linear regression?\n",
    "\n",
    ">Time permitting, repeat the training and testing with three predictors and calculate the mean squared errors.  How do these compare to the errors from the one and two predictor models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23.23966191  16.73372632  19.08920416  23.89016639  14.74945857\n",
      "  12.86532856  21.18878619  17.0114931   15.79863916  20.7890107 ]\n",
      "[-3.88585404 -0.03259234]\n",
      "37.6562613676\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "X_train = traindf[['wt','hp']]\n",
    "X_test = testdf[['wt','hp']]\n",
    "ling_mul = linear_model.LinearRegression().fit(X_train,y_train)\n",
    "y_pred = ling_mul.predict(X_test)\n",
    "print(y_pred)\n",
    "print(ling_mul.coef_)\n",
    "print(ling_mul.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.86972256277\n",
      "4.65941745382\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_train, ling_mul.predict(X_train)))\n",
    "print(mean_squared_error(y_test, ling_mul.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lin_sm = sm.OLS(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   19.59</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 25 Oct 2017</td> <th>  Prob (F-statistic):</th> <td>1.95e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:06:33</td>     <th>  Log-Likelihood:    </th> <td> -87.469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    22</td>      <th>  AIC:               </th> <td>   178.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    20</td>      <th>  BIC:               </th> <td>   181.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wt</th> <td>    8.2680</td> <td>    5.854</td> <td>    1.412</td> <td> 0.173</td> <td>   -3.942    20.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hp</th> <td>   -0.0708</td> <td>    0.142</td> <td>   -0.500</td> <td> 0.622</td> <td>   -0.366     0.225</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.118</td> <th>  Durbin-Watson:     </th> <td>   1.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.943</td> <th>  Jarque-Bera (JB):  </th> <td>   0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.100</td> <th>  Prob(JB):          </th> <td>   0.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.438</td> <th>  Cond. No.          </th> <td>    281.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.662\n",
       "Model:                            OLS   Adj. R-squared:                  0.628\n",
       "Method:                 Least Squares   F-statistic:                     19.59\n",
       "Date:                Wed, 25 Oct 2017   Prob (F-statistic):           1.95e-05\n",
       "Time:                        15:06:33   Log-Likelihood:                -87.469\n",
       "No. Observations:                  22   AIC:                             178.9\n",
       "Df Residuals:                      20   BIC:                             181.1\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "wt             8.2680      5.854      1.412      0.173        -3.942    20.478\n",
       "hp            -0.0708      0.142     -0.500      0.622        -0.366     0.225\n",
       "==============================================================================\n",
       "Omnibus:                        0.118   Durbin-Watson:                   1.835\n",
       "Prob(Omnibus):                  0.943   Jarque-Bera (JB):                0.326\n",
       "Skew:                          -0.100   Prob(JB):                        0.850\n",
       "Kurtosis:                       2.438   Cond. No.                         281.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_sm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23.23966191  16.73372632  19.08920416  23.89016639  14.74945857\n",
      "  12.86532856  21.18878619  17.0114931   15.79863916  20.7890107 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 21.98303964,   7.50903653,  18.47799878,  13.87022701,\n",
       "        14.39443421,   5.78694588,  10.50615485,  19.39421496,\n",
       "        12.16208423,  21.16949018])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_pred)\n",
    "lin_sm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4: Interpreting results [5 minutes / remaining time]\n",
    "Tell a story with your results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We observe that both the training MSE and testing MSE are lower than the corresponding training and testing MSEs when we had one predictor. So it seems that we'd prefer our model with two predictors than one predictor.  \n",
    "But we observe that with the two predictor model the training MSE is larger than the testing MSE. It is possible for the training MSE to be higher than the testing MSE(Bias).One possibility is that the test set was too small, so that the model by chance fit it better than the training set."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
